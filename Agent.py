import numpy as np

class stupidAgent():
    def __init__(self, stepLimit:int, number_of_bandits:int, epsilon:float, UCB_constant:float) -> None:
        self.stepLimit = stepLimit
        self.curr_step = 1.0
        self.num_of_bandit = number_of_bandits   
        self.action_method = None  
        self.epsilon = epsilon
        self.E_ratio = [0, 0]       
        self.UCB_constant = UCB_constant  
        self.reward = np.zeros(number_of_bandits)  
        self.reward_history = np.array([self.reward])   
        self.actionVaule = np.zeros(number_of_bandits)
        self.actionVaule_history = np.array([self.actionVaule])
        self.actionCount = np.zeros(number_of_bandits)
        self.bestAction = np.random.randint(number_of_bandits, size=1)  #initial action
        self.temp = np.zeros(number_of_bandits)
        self.ess = np.zeros(number_of_bandits)
        
    '''==================================< Class' Methods >============================================='''
    def update(self, action:int, reward:float)->None:
        self.curr_step += 1.0
        self.reward[action] += reward
        self.actionCount[action] += 1.0
        self.actionVaule[action] = self.actionVaule[action] + (reward - self.actionVaule[action])/self.actionCount[action]
        self.update_history()
    
    def update_history(self)->None:
        self.reward_history = np.concatenate((self.reward_history,np.array([self.reward])), axis=0)
        self.actionVaule_history = np.concatenate((self.actionVaule_history,np.array([self.actionVaule])), axis=0)
    
    def epsilon_greedy(self, epsilon:float)->int:
        decision = np.random.rand()
        if decision <= epsilon:
            self.E_ratio[0] += 1
            selected_action = np.random.randint(self.num_of_bandit, size=1)
        else:
            self.E_ratio[1] += 1
            max_vaule = max(self.actionVaule)
            max_ind = []
            for i in range(self.num_of_bandit):
                if self.actionVaule[i] == max_vaule:
                    max_ind.append(i)
            selected_action = np.random.choice(max_ind)
            self.bestAction = selected_action
        return selected_action
    
    def UCB(self, constant:float)->int:
        import math
        for i in range(self.num_of_bandit):
            t = self.curr_step
            n = self.actionCount[i] + 1.0
            Q = self.actionVaule[i]
            U = constant*math.sqrt(math.log(t)/n)
            self.temp[i] = Q + U
        max_vaule = max(self.temp)
        max_ind = []
        for i in range(self.num_of_bandit):
            if self.temp[i] == max_vaule:
                max_ind.append(i)
        selected_action = np.random.choice(max_ind)
        self.bestAction = selected_action        
        return selected_action
    
    def action_selection(self, method:str)->int:
        match method:
            case "greedy":
                self.action_method = "greedy"
                selected_action = self.epsilon_greedy(self.epsilon)
            case "UCB":
                self.action_method = "UCB"
                selected_action = self.UCB(self.UCB_constant)
        return selected_action
    
    def calculate_ess(self, true_dis:list[float])->None:
         for i in range(self.num_of_bandit):
            self.ess[i] = true_dis[i] - self.actionVaule[i]
    
    def debug(self)->None:
        match self.action_method:
            case "greedy":
                print(" ")
                print(">-----The result from epsilon-greedy-----<")
                print("Explolation-Exploitation counts :", self.E_ratio)
                print("Used epsilon : ", self.epsilon)
            case "UCB":
                print(" ")
                print(">-----The result from Upper Cofidence Bound-----<")
                print("Used UCB's constant : ", self.UCB_constant)
        print("Final step action value : ", self.actionVaule)
        print("Final step action value errors: ", self.ess)
        print("Final step action counts : ", self.actionCount)
        print("Best action, Bandit number(start with 0) : ", self.bestAction)
                
    def plot_history(self, name:str, true_dis:list[float])->None:
        import matplotlib.pyplot as plt
        match name:
            case "action_value":
                data = self.actionVaule_history
                y_label = "Action value"
                title = "Action value VS time"
                plt.plot(max(true_dis)*np.ones(int(self.curr_step) - 1), 'g', linestyle='dotted', label = "maximum reward change")
                # plt.plot(true_dis[self.bestAction]*np.ones(int(self.curr_step) - 1), 'k', linestyle='dashed', label = "reward change of best action")
                for i in range(self.num_of_bandit):
                    plt.plot(true_dis[i]*np.ones(int(self.curr_step) - 1), 'k', linestyle='dashed')
                    plt.plot(data[:,i],label=str(i))   
            case "reward":
                data = self.reward_history
                y_label = "Reward"
                title = "Reward VS time"                
                for i in range(self.num_of_bandit):
                    plt.plot(data[:,i],label="bandit"+str(i))
        plt.xlabel("Time(step)")
        plt.ylabel(y_label)
        plt.title(title+" - "+self.action_method)
        plt.legend()
        plt.show()
    
    def plot_actionCount(self)->None:
        import matplotlib.pyplot as plt
        bd = []
        for i in range(self.num_of_bandit):
            bd.append("bandit #"+str(i))
        plt.bar(bd,self.actionCount)
        # Adding labels and title
        plt.xlabel('Bandit')
        plt.ylabel('Frequency')
        plt.title('Bandit Usage Frequency')
        plt.xticks(rotation=45)
        plt.tight_layout()  # Adjust layout to prevent clipping of labels
        plt.show()          
           
        